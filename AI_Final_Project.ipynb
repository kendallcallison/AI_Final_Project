{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kendallcallison/AI_Final_Project/blob/main/AI_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ],
      "metadata": {
        "id": "uxgBX0YXu1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "# Define function to split dataset into train and test sets\n",
        "def train_test_split(data, test_size = 0.2):\n",
        "    data = data.sample(frac = 1).reset_index(drop = True)  # Shuffle the data\n",
        "    split_idx = int(len(data) * (1 - test_size))\n",
        "    train_data = data.iloc[:split_idx]\n",
        "    test_data = data.iloc[split_idx:]\n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "5toC7iy9btjg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "FXyRfd35yRPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to separate data by class\n",
        "def nb_separate_by_class(data):\n",
        "    separated = {}\n",
        "    for _, row in data.iterrows():\n",
        "        label = row['spam']\n",
        "        if label not in separated:\n",
        "            separated[label] = []\n",
        "        separated[label].append(row.drop('spam'))\n",
        "    return separated\n",
        "\n",
        "# Define function to calculate mean and standard deviation for each feature\n",
        "def nb_summarize(data):\n",
        "    summaries = {}\n",
        "    for label, rows in data.items():\n",
        "        features = np.array([row[:-1] for row in rows])\n",
        "        summaries[label] = [(np.mean(column), np.std(column)) for column in features.T]\n",
        "    return summaries\n",
        "\n",
        "# Define function to calculate Gaussian probability density function\n",
        "def nb_gaussian_probability(x, mean, std):\n",
        "    exponent = np.exp(-((x - mean) ** 2 / (2 * std ** 2)))\n",
        "    return (1 / (np.sqrt(2 * np.pi) * std)) * exponent\n",
        "\n",
        "# Define function to calculate class probabilities\n",
        "def nb_class_probabilities(summaries, input_data):\n",
        "    probabilities = {}\n",
        "    for label, class_summaries in summaries.items():\n",
        "        probabilities[label] = 1\n",
        "        for i in range(len(class_summaries)):\n",
        "            mean, std = class_summaries[i]\n",
        "            x = input_data[i]\n",
        "            probabilities[label] *= nb_gaussian_probability(x, mean, std)\n",
        "    return probabilities\n",
        "\n",
        "# Define function to predict the class for a given input\n",
        "def nb_predict(summaries, input_data):\n",
        "    probabilities = nb_class_probabilities(summaries, input_data)\n",
        "    best_label, best_prob = None, -1\n",
        "    for label, probability in probabilities.items():\n",
        "        if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = label\n",
        "    return best_label\n",
        "\n",
        "# Define function to evaluate the model\n",
        "def evaluate_nb(train_data, test_data):\n",
        "    separated = nb_separate_by_class(train_data)\n",
        "    summaries = nb_summarize(separated)\n",
        "    predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        input_data = test_data.iloc[i][:-1]\n",
        "        label = nb_predict(summaries, input_data)\n",
        "        predictions.append(label)\n",
        "    actual_labels = test_data['spam'].tolist()\n",
        "    correct = sum(1 for i in range(len(predictions)) if predictions[i] == actual_labels[i])\n",
        "    accuracy = correct / float(len(predictions)) * 100.0\n",
        "    return accuracy\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size = 0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluate_nb(train_data, test_data)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "vzIRekbk7EUr",
        "outputId": "85f8866f-b5a6-446b-9a90-1bfdcb54dcc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 82.19326818675353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ],
      "metadata": {
        "id": "5jRvHTlW0DYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate cosine similarity using numpy\n",
        "def cosine_similarity(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_product = np.linalg.norm(a) * np.linalg.norm(b)\n",
        "    return dot_product / norm_product\n",
        "\n",
        "# Define function to calculate K Nearest Neighbors\n",
        "def k_nearest_neighbors(train_data, val_row, k=3):\n",
        "    similarities = train_data.iloc[:, :-1].apply(lambda row: cosine_similarity(row, val_row[:-1]), axis=1)\n",
        "    neighbors = list(zip(train_data['spam'], similarities))\n",
        "    neighbors.sort(key=lambda x: x[1], reverse=True)\n",
        "    return neighbors[:k]\n",
        "\n",
        "# Define function to predict class based on K Nearest Neighbors\n",
        "def predict_class(neighbors):\n",
        "    labels = [neighbor[0] for neighbor in neighbors]\n",
        "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
        "    return most_common_label\n",
        "\n",
        "# Define function to evaluate the model\n",
        "def evaluate_knn(train_data, val_data, k=3):\n",
        "    predictions = []\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        for i, val_row in val_data.iterrows():\n",
        "            neighbors = k_nearest_neighbors(train_data, val_row, k)\n",
        "            prediction = predict_class(neighbors)\n",
        "            predictions.append(prediction)\n",
        "    actual_labels = val_data['spam'].tolist()\n",
        "    correct = sum(1 for i in range(len(predictions)) if predictions[i] == actual_labels[i])\n",
        "    accuracy = correct / float(len(predictions)) * 100.0\n",
        "    return accuracy\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_data, val_data = train_val_split(data, val_size = 0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluate_knn(train_data, test_data)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYjTznL8cN1H",
        "outputId": "e1790783-0d8a-4219-9dbf-b286b9c5864d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 89.35939196525516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ],
      "metadata": {
        "id": "OUzUupva0Fxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sigmoid function with numerical stability\n",
        "def lr_sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "# Define function to initialize weights\n",
        "def lr_initialize_weights(dim):\n",
        "    return np.zeros((dim, 1))\n",
        "\n",
        "# Define function to compute cost and gradients\n",
        "def lr_compute_cost_and_gradients(X, y, weights):\n",
        "    m = len(y)\n",
        "    z = np.dot(X, weights)\n",
        "    h = lr_sigmoid(z)\n",
        "    epsilon = 1e-10  # Small epsilon to avoid taking the logarithm of zero\n",
        "    cost = -np.mean(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
        "    gradient = np.dot(X.T, (h - y)) / m\n",
        "    return cost, gradient\n",
        "\n",
        "# Define function to train logistic regression model\n",
        "def lr_train_logistic_regression(X, y, learning_rate=0.01, num_iterations=100):\n",
        "    m, n = X.shape\n",
        "    weights = lr_initialize_weights(n)\n",
        "    for i in range(num_iterations):\n",
        "        cost, gradient = lr_compute_cost_and_gradients(X, y, weights)\n",
        "        weights -= learning_rate * gradient\n",
        "    return weights\n",
        "\n",
        "# Define function to predict using logistic regression model\n",
        "def lr_predict(X, weights):\n",
        "    z = np.dot(X, weights)\n",
        "    return lr_sigmoid(z)\n",
        "\n",
        "# Main function\n",
        "def evaluate_lr(data):\n",
        "    # Split the dataset into features (X) and labels (y)\n",
        "    X = data.drop(columns=['spam']).values\n",
        "    y = data['spam'].values.reshape(-1, 1)\n",
        "\n",
        "    # Split the data into train and test sets (80% train, 20% test)\n",
        "    split_idx = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    # Train logistic regression model\n",
        "    weights = lr_train_logistic_regression(X_train, y_train)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    predictions = lr_predict(X_test, weights)\n",
        "    predictions = (predictions >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predictions == y_test) * 10000\n",
        "    return accuracy\n",
        "\n",
        "evaluate_lr(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4W_lp4BgcN0",
        "outputId": "2fb7e2bd-aac6-472c-ed63-fcbe8f5fdcc0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76.0043431053203"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ],
      "metadata": {
        "id": "mAssSW_I0GvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def performance(model, data):\n",
        "  result = 0\n",
        "\n",
        "  if (model == \"nb\"):\n",
        "    print(\"nb \", end='')\n",
        "    result = evaluate_nb(train_data, test_data)\n",
        "\n",
        "  if (model == \"knn\"):\n",
        "    print(\"knn \", end='')\n",
        "    result = evaluate_knn(train_data, test_data)\n",
        "\n",
        "  if (model == \"lr\"):\n",
        "    print(\"lr \", end='')\n",
        "    result = evaluate_lr(data)\n",
        "\n",
        "  print(\"result: \" + str(result) + \"%\")\n",
        "\n",
        "  return\n",
        "\n",
        "# run each algorithm and compare the results\n",
        "performance(\"nb\", data)\n",
        "performance(\"knn\", data)\n",
        "performance(\"lr\", data)"
      ],
      "metadata": {
        "id": "e0MQ0eo0MnmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321eccef-d641-45bb-d0c1-3463475cb003"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb result: 83.27904451682954%\n",
            "knn result: 89.35939196525516%\n",
            "lr result: 76.0043431053203%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76.0043431053203"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t-O8VVfVpnP4"
      }
    }
  ]
}